{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "995013bf-9299-43c9-a093-49547fa0740c",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "1. [Loading the Datasets](#firstbullet)\n",
    "2. [Exploratory Data Analysis](#secondbullet)\n",
    "3. [Splitting the Dataset](#thirdbullet)\n",
    "4. [Canonicalizing the Datast](#fourthbullet)\n",
    "5. [Removing the Atom Maps](#fifthbullet)\n",
    "6. [Tokenizing the Dataset](#sixthbullet)\n",
    "7. [Preparing the DataFrames](#seventhbullet)\n",
    "8. [Shuffling and Saving](#eighthbullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b712a3-5798-487d-aa59-2cfbca0893b4",
   "metadata": {},
   "source": [
    "### 1. Loading the datasets: Exploratory data analysis <a class=\"anchor\" id=\"firstbullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19ee965-54a5-4192-979a-40e6e6e5c4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"C:\\Users\\sayye\\OneDrive\\Documents\\New folder (2)\\USPTO 50k dataset\\uspto50k_train.csv\")\n",
    "test = pd.read_csv(r\"C:\\Users\\sayye\\OneDrive\\Documents\\New folder (2)\\USPTO 50k dataset\\uspto50k_test.csv\")\n",
    "val = pd.read_csv(r\"C:\\Users\\sayye\\OneDrive\\Documents\\New folder (2)\\USPTO 50k dataset\\uspto50k_val.csv\")\n",
    "\n",
    "#a glimpse of the datasets\n",
    "print(train.head(),'\\n\\n', val.head(), '\\n\\n', test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d578825d-d744-4a27-8094-d2212a90e2f8",
   "metadata": {},
   "source": [
    "### 2. Exploratory Data Analysis <a class=\"anchor\" id=\"secondbullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60be449-f41e-4c79-b3ae-fd11adce6d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape of the train dataset:\\t{train.shape}\\nShape of the validation dataset:\\t{val.shape}\\nShape of the test dataset:\\t{test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3e9d8c-d216-4566-a20c-83e4e062dde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Info of the train dataset:\\t{train.describe()}\\nInfo of the validation dataset:\\t{val.describe()}\\nInfo of the test dataset:\\t{test.describe()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada9badb-b427-42c4-b15d-f6f56e4dfa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Columns in the train dataset:\\t{train.columns}\\nColumns in the validation dataset:\\t{val.columns}\\nColumns in the test dataset:\\t{test.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20612d5-021c-4e9c-b041-229bf48be657",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['source'] = 'train'\n",
    "val['source'] = 'val'\n",
    "test['source'] = 'test'\n",
    "\n",
    "df_all = pd.concat([train, val, test], ignore_index=True)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba41bab9-0c8a-41bd-804d-65e5334ac739",
   "metadata": {},
   "source": [
    "Since the datsets are open source, they are pretty much clean and don't have null values. We have explored the datasets as per our requirements of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1753aa6-67c2-4a24-8ea8-eb3776bc374e",
   "metadata": {},
   "source": [
    "### 3. Splitting the Dataset <a class=\"anchor\" id=\"thirdbullet\"></a>\n",
    "\n",
    "As shown the dataset has three columns having the id of the reaction, the class, i.e., the type of the reaction, be it elimination, substitution encoded, along with the overall reaction. It's better for the reaction to be split into their components i.e., reactants, reagents and products for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf81ab-6784-4a7e-8dfa-271f1fa2f876",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['reactants', 'reagents', 'products']] = train['reactants>reagents>production'].str.split('>', expand = True)\n",
    "test[['reactants', 'reagents', 'products']] = test['reactants>reagents>production'].str.split('>', expand = True)\n",
    "val[['reactants', 'reagents', 'products']] = val['reactants>reagents>production'].str.split('>', expand = True)\n",
    "\n",
    "#checking the columns\n",
    "print(train.columns, '\\n', val.columns, '\\n', test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4c7a10-0ca8-4d44-a1e2-494af7443eb1",
   "metadata": {},
   "source": [
    "### 4. Canonicalizing the Data <a class=\"anchor\" id=\"fourthbullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f327bef2-cd87-4398-a196-c20c5a820e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize(smiles): # will raise an Exception if invalid SMILES\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        return Chem.MolToSmiles(mol)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "train['reactants'] = train['reactants'].apply(canonicalize)\n",
    "train['reagents'] = train['reagents'].apply(canonicalize)\n",
    "train['products'] = train['products'].apply(canonicalize)\n",
    "\n",
    "test['reactants'] = test['reactants'].apply(canonicalize)\n",
    "test['reagents'] = test['reagents'].apply(canonicalize)\n",
    "test['products'] = test['products'].apply(canonicalize)\n",
    "\n",
    "val['reactants'] = val['reactants'].apply(canonicalize)\n",
    "val['reagents'] = val['reagents'].apply(canonicalize)\n",
    "val['products'] = val['products'].apply(canonicalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0c7442-b75c-4a74-8578-8228e24ac065",
   "metadata": {},
   "source": [
    "### 5. Removing the Atom maps <a class=\"anchor\" id=\"fifthbullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8094b30e-26dd-4c3b-9ca9-b3878ba1822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_atommapping(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        for atom in mol.GetAtoms():\n",
    "            atom.SetAtomMapNum(0)\n",
    "        return Chem.MolToSmiles(mol)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "train['reactants'] = train['reactants'].apply(remove_atommapping)\n",
    "train['reagents'] = train['reagents'].apply(remove_atommapping)\n",
    "train['products'] = train['products'].apply(remove_atommapping)\n",
    "\n",
    "test['reactants'] = test['reactants'].apply(remove_atommapping)\n",
    "test['reagents'] = test['reagents'].apply(remove_atommapping)\n",
    "test['products'] = test['products'].apply(remove_atommapping)\n",
    "\n",
    "val['reactants'] = val['reactants'].apply(remove_atommapping)\n",
    "val['reagents'] = val['reagents'].apply(remove_atommapping)\n",
    "val['products'] = val['products'].apply(remove_atommapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1113b3b4-5f7f-4977-b0a9-1aebc8611f5f",
   "metadata": {},
   "source": [
    "### 6. Tokenizing the Dataset <a class=\"anchor\" id=\"sixthbullet\"></a>\n",
    "\n",
    "To be able to train a language model, we need to split the strings into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3810902b-f4dc-47c7-a775-feeeb8fbdb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGEX_TOKENIZER =  r\"(\\%\\([0-9]{3}\\)|\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\||\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>>?|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "\n",
    "def tokenize(smiles):\n",
    "    return ' '.join(smiles)\n",
    "\n",
    "train['token_reactants'] = train['reactants'].apply(tokenize)\n",
    "train['token_reagents'] = train['reagents'].apply(tokenize)\n",
    "train['token_products'] = train['products'].apply(tokenize)\n",
    "\n",
    "test['token_reactants'] = test['reactants'].apply(tokenize)\n",
    "test['token_reagents'] = test['reagents'].apply(tokenize)\n",
    "test['token_products'] = test['products'].apply(tokenize)\n",
    "\n",
    "val['token_reactants'] = val['reactants'].apply(tokenize)\n",
    "val['token_reagents'] = val['reagents'].apply(tokenize)\n",
    "val['token_products'] = val['products'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9a9961-93ae-44f3-a586-443a65a87170",
   "metadata": {},
   "source": [
    "### Preparing the Data Frames <a class=\"anchor\" id=\"seventhbullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec3cc3-8117-4786-970d-1e75269134f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({'Id': train['id'], \n",
    "                         'Class': train['class'], \n",
    "                         'Tokenized Reactants': train['token_reactants'],\n",
    "                         'Tokenized Products': train['token_products'],\n",
    "                        'Overall Reaction': train['reactants>reagents>production']})\n",
    "print(f\"The training set contains {train_df.shape[0]} reactions.\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf445b87-3f19-4f51-abf5-27a971f1013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({'Id': test['id'], \n",
    "                        'Class': test['class'],\n",
    "                        'Tokenized Reactants': test['token_reactants'],\n",
    "                        'Tokenized Products': test['token_products'],\n",
    "                       'Overall Reaction': test['reactants>reagents>production']})\n",
    "print(f\"The training set contains {test_df.shape[0]} reactions.\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2eba3c-e1ab-4fc9-9cb5-94781ad5fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.DataFrame({'Id': val['id'],\n",
    "                       'Class': val['class'],\n",
    "                       'Tokenized Reactants': val['token_reactants'],\n",
    "                       'Tokenized Products': val['token_products'],\n",
    "                      'Overall Reaction': val['reactants>reagents>production']})\n",
    "print(f\"The training set contains {val_df.shape[0]} reactions.\")\n",
    "val_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529fa3b6-1b81-48b0-9384-0c7199f36418",
   "metadata": {},
   "source": [
    "### Shuffling and saving the datasets <a class=\"anchor\" id=\"eighthbullet\"></a>\n",
    "\n",
    "The dataset contains different types of reactions arranged in a ordered manner (as shown the snippet has same type of reaction i.e., 5) hence, without shuffling model might learn patterns that are not generalizable. Shuffling ensures that each training batch has a variety of reaction types, reactants, and complexities. This helps the model learn general rules of reactivity, and avoid overfitting. After shuffling it can be seen the dataset is random from the classof the reaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d17fd7d-c5f0-48cc-a632-530f43f8642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rn = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_rn = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "val_rn = val_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "train_rn.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ab02f-4a13-4c04-8a44-d0728a8f7105",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rn['Tokenized Reactants'].to_csv(\"uspto_50k_train_reactants.txt\", index=False, header=False)\n",
    "train_rn['Tokenized Products'].to_csv(\"uspto_50k_train_products.txt\", index=False, header=False)\n",
    "\n",
    "test_rn['Tokenized Reactants'].to_csv(\"uspto_50k_test_reactants.txt\", index=False, header=False)\n",
    "test_rn['Tokenized Products'].to_csv(\"uspto_50k_test_products.txt\", index=False, header=False)\n",
    "\n",
    "val_rn['Tokenized Reactants'].to_csv(\"uspto_50k_val_reactants.txt\", index=False, header=False)\n",
    "val_rn['Tokenized Products'].to_csv(\"uspto_50k_val_products.txt\", index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
